{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"torchml implements the scikit-learn API on top of PyTorch. This means we automatically get GPU support for scikit-learn and, when possible, differentiability. Resources \u00b6 GitHub: github.com/learnables/torchml Documentation: learnables.net/torchml Tutorials: learnables.net/torchml/tutorials Examples: learnables.net/torchml/examples Getting Started \u00b6 pip install torchml Minimal Linear Regression Example \u00b6 import torchml as ml ( X_train , y_train ), ( X_test , y_test ) = generate_data () # API closely follows scikit-learn linreg = ml . linear_model . LinearRegression () linreg . fit ( X_train , y_train ) linreg . predict ( X_test ) Changelog \u00b6 A human-readable changelog is available in the CHANGELOG.md file. Citing \u00b6 To cite torchml repository in your academic publications, please use the following reference. S\u00e9bastien M. R. Arnold, Lucy Xiaoyang Shi, Xinran Gao, Zhiheng Zhang, and Bairen Chen. 2023. \"torchml: a scikit-learn implementation on top of PyTorch\". You can also use the following Bibtex entry: @misc { torchml , author = {Arnold, S{\\'e}bastien M R and Shi, Lucy Xiaoyang and Gao, Xinran and Zhang, Zhiheng and Chen, Bairen} , title = {torchml: A scikit-learn implementation on top of PyTorch} , year = {2023} , url = {https://github.com/learnables/torchml} , }","title":"Home"},{"location":"#resources","text":"GitHub: github.com/learnables/torchml Documentation: learnables.net/torchml Tutorials: learnables.net/torchml/tutorials Examples: learnables.net/torchml/examples","title":"Resources"},{"location":"#getting-started","text":"pip install torchml","title":"Getting Started"},{"location":"#minimal-linear-regression-example","text":"import torchml as ml ( X_train , y_train ), ( X_test , y_test ) = generate_data () # API closely follows scikit-learn linreg = ml . linear_model . LinearRegression () linreg . fit ( X_train , y_train ) linreg . predict ( X_test )","title":"Minimal Linear Regression Example"},{"location":"#changelog","text":"A human-readable changelog is available in the CHANGELOG.md file.","title":"Changelog"},{"location":"#citing","text":"To cite torchml repository in your academic publications, please use the following reference. S\u00e9bastien M. R. Arnold, Lucy Xiaoyang Shi, Xinran Gao, Zhiheng Zhang, and Bairen Chen. 2023. \"torchml: a scikit-learn implementation on top of PyTorch\". You can also use the following Bibtex entry: @misc { torchml , author = {Arnold, S{\\'e}bastien M R and Shi, Lucy Xiaoyang and Gao, Xinran and Zhang, Zhiheng and Chen, Bairen} , title = {torchml: A scikit-learn implementation on top of PyTorch} , year = {2023} , url = {https://github.com/learnables/torchml} , }","title":"Citing"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6 0.1.0 \u00b6 Added \u00b6 Initial release with support for linear models (LinearRegression, Lasso, Ridge), naive Bayes (GaussianNaiveBayes), neighbor methods (NearestCentroid, NearestNeighbors, KNeighborsClassifier), discriminant analysis (LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis), decompositions (PCA), and kernel approximations (RBFSampler). Most methods are differentiable (and gradient checked!). Support for CPU/GPU. Several tutorials. Changed \u00b6 Fixed \u00b6","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"changelog/#010","text":"","title":"0.1.0"},{"location":"changelog/#added_1","text":"Initial release with support for linear models (LinearRegression, Lasso, Ridge), naive Bayes (GaussianNaiveBayes), neighbor methods (NearestCentroid, NearestNeighbors, KNeighborsClassifier), discriminant analysis (LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis), decompositions (PCA), and kernel approximations (RBFSampler). Most methods are differentiable (and gradient checked!). Support for CPU/GPU. Several tutorials.","title":"Added"},{"location":"changelog/#changed_1","text":"","title":"Changed"},{"location":"changelog/#fixed_1","text":"","title":"Fixed"},{"location":"getting_start/","text":"torchml \u00b6 TODOs \u00b6 Replace myproject with torchml everywhere such that: All tests pass. The docs render nicely. Implement algorithms from scikit-learn with PyTorch. including tests for feature parity (ie, same API, same predictions) and gradient correctness (ie, finite differences). including docs. preliminary example: Linear Regression with some tests . A logo? Getting started \u00b6 Create a virtual environment: conda create -n torchml python=3.9 conda activate torchml Install the library locally in development mode: make dev Tests \u00b6 Add your own unit tests in: tests/unit/module/submodule_test.py and run: make tests Docs \u00b6 Add your own to: mkdocs.yaml and in docs/api/module.md , run: make docs and open http://localhost:8000 .","title":"torchml"},{"location":"getting_start/#torchml","text":"","title":"torchml"},{"location":"getting_start/#todos","text":"Replace myproject with torchml everywhere such that: All tests pass. The docs render nicely. Implement algorithms from scikit-learn with PyTorch. including tests for feature parity (ie, same API, same predictions) and gradient correctness (ie, finite differences). including docs. preliminary example: Linear Regression with some tests . A logo?","title":"TODOs"},{"location":"getting_start/#getting-started","text":"Create a virtual environment: conda create -n torchml python=3.9 conda activate torchml Install the library locally in development mode: make dev","title":"Getting started"},{"location":"getting_start/#tests","text":"Add your own unit tests in: tests/unit/module/submodule_test.py and run: make tests","title":"Tests"},{"location":"getting_start/#docs","text":"Add your own to: mkdocs.yaml and in docs/api/module.md , run: make docs and open http://localhost:8000 .","title":"Docs"},{"location":"api/","text":"torchml \u00b6","title":"torchml"},{"location":"api/#torchml","text":"","title":"torchml"},{"location":"api/decomposition/","text":"======= torchml.decomposition \u00b6 Classes \u00b6 torchml.decomposition.PCA \u00b6 Description \u00b6 Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD. References \u00b6 The scikit-learn [documentation page] (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009. Arguments \u00b6 n_components : int, default=None Number of components to keep. if n_components is not set all components are kept:: n_components == min(n_samples, n_features) svd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto' The algorithm that runs SVD. If auto : The solver is selected by a default policy based on X.shape and n_components : if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient 'randomized' method is enabled. Otherwise the exact full SVD is computed. Example \u00b6 import torch from torchml.decomposition import PCA X = torch . tensor ([[ - 1 , - 1 ], [ - 2 , - 1 ], [ - 3 , - 2 ], [ 1 , 1 ], [ 2 , 1 ], [ 3 , 2 ]]) pca = PCA ( n_components = 2 ) pca . fit ( X ) fit ( self , X ) \u00b6 Description \u00b6 Fit the model with X. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 pca = PCA () pca . fit ( X ) fit_transform ( self , X ) \u00b6 Description \u00b6 Fit the model with X and apply the dimensionality reduction on X. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 pca = PCA () X_reduced = pca . fit_transform ( X ) transform ( self , X ) \u00b6 Description \u00b6 Apply dimensionality reduction to X. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 pca = PCA () X_reduced = pca . fit ( X ) . transform ( X )","title":"torchml.decomposition"},{"location":"api/decomposition/#torchmldecomposition","text":"","title":"torchml.decomposition"},{"location":"api/decomposition/#classes","text":"","title":"Classes"},{"location":"api/decomposition/#torchml.decomposition.PCA","text":"","title":"PCA"},{"location":"api/decomposition/#torchml.decomposition.PCA.fit","text":"","title":"fit()"},{"location":"api/decomposition/#torchml.decomposition.PCA.fit_transform","text":"","title":"fit_transform()"},{"location":"api/decomposition/#torchml.decomposition.PCA.transform","text":"","title":"transform()"},{"location":"api/discriminant_analysis/","text":"torchml.discriminant_analysis \u00b6 Classes \u00b6 torchml.discriminant_analysis.LinearDiscriminantAnalysis \u00b6 [Source] Description \u00b6 Linear Discriminant Analysis is a classifier with a linear decision boundary, which is calculated by fitting class conditional densities to the data and using Bayes' rule. This model fits a Gaussian density to each class and it assumes that all classes share the same covariance matrix. This current implementation only includes \"svd\" solver. References \u00b6 Linear discriminant analysis : a detailed tutorial tutorial The scikit-learn documentation page Arguments \u00b6 n_components (int, default=None) - Number of components (features) for dimensionality reduction. If None, will be set to min(n_classes - 1, n_features). priors (torch.Tensor, default=None) - The class prior probabilities. By default, the class proportions are calculated from the input training data. tol (float, default=1e-4) - Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Used only in \"svd\" solver. solver (str, default=\"svd\") - Solver to use. Currently only support \"svd\" solver. Example \u00b6 lda = LinearDiscriminantAnalysis () fit ( self , X : Tensor , y : Tensor ) \u00b6 Description \u00b6 Fit the Linear Discriminant Analysis model. Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates. Example \u00b6 lda = LinearDiscriminantAnalysis () lda . fit ( X_train , y_train ) predict ( self , X : Tensor ) \u00b6 Description \u00b6 Predict using Linear Discriminant Analysis model. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 lda = LinearDiscriminantAnalysis () lda . fit ( X_train , y_train ) lda . predict ( X_test ) transform ( self , X : Tensor ) \u00b6 Description \u00b6 Transform and project data to maximize class separation. Arguments \u00b6 X (Tensor) - Input data. Example \u00b6 lda = LinearDiscriminantAnalysis () lda . fit ( X_train , y_train ) lda . transform ( X_test ) torchml.discriminant_analysis.QuadraticDiscriminantAnalysis \u00b6 [Source] Description \u00b6 Quadratic Discriminant Analysis is a classifier with a quadratic decision boundary, which is calculated by fitting class conditional densities to the data and using Bayes' rule. This model fits a Gaussian density to each class. This current implementation only includes \"svd\" solver. References \u00b6 Carl J Huberty's Discriminant Analysis paper The scikit-learn documentation page Arguments \u00b6 priors (torch.Tensor, default=None) - The class prior probabilities. By default, the class proportions are calculated from the input training data. reg_param (float, default=0.0) - Regularizes the per-class covariance estimates by transforming S2 as S2 = ((1 - reg_param) * S2) + reg_param , where S2 corresponds to the scaling_ attribute of a given class. store_covariance (bool, default=False) - If True, the class covariance matrices will be explicitly computed and stored in the self.covariance_ attribute. tol (float, default=1e-4) - Absolute threshold for a singular value to be considered significant. This parameter does not affect the predictions. It controls a warning that is raised when features are considered to be colinear. Example \u00b6 qda = QuadraticDiscriminantAnalysis () decision_function ( self , X : Tensor ) \u00b6 Description \u00b6 Apply decision function to an array of samples. Arguments \u00b6 X (Tensor) - Input data. Example \u00b6 qda = QuadraticDiscriminantAnalysis () qda . fit ( X_train , y_train ) qda_dec_func = qda . decision_function ( X_test ) fit ( self , X : Tensor , y : Tensor ) \u00b6 Description \u00b6 Fit the Quadratic Discriminant Analysis model. Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates. Example \u00b6 qda = QuadraticDiscriminantAnalysis () qda . fit ( X_train , y_train ) predict ( self , X : Tensor ) \u00b6 Description \u00b6 Predict using Quadratic Discriminant Analysis model. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 qda = QuadraticDiscriminantAnalysis () qda . fit ( X_train , y_train ) qda_pred = qda . predict ( X_test ) predict_log_proba ( self , X : Tensor ) \u00b6 Description \u00b6 Calculate and return log of posterior probabilities of classification. Arguments \u00b6 X (Tensor) - Input data. Example \u00b6 qda = QuadraticDiscriminantAnalysis () qda . fit ( X_train , y_train ) qda_predict_log_proba = qda . predict_log_proba ( X_test ) predict_proba ( self , X : Tensor ) \u00b6 Description \u00b6 Calculate and return posterior probabilities of classification. Arguments \u00b6 X (Tensor) - Input data. Example \u00b6 qda = QuadraticDiscriminantAnalysis () qda . fit ( X_train , y_train ) qda_predict_proba = qda . predict_proba ( X_test )","title":"torchml.discriminant_analysis"},{"location":"api/discriminant_analysis/#torchmldiscriminant_analysis","text":"","title":"torchml.discriminant_analysis"},{"location":"api/discriminant_analysis/#classes","text":"","title":"Classes"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.LinearDiscriminantAnalysis","text":"[Source]","title":"LinearDiscriminantAnalysis"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.LinearDiscriminantAnalysis.fit","text":"","title":"fit()"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.LinearDiscriminantAnalysis.predict","text":"","title":"predict()"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.LinearDiscriminantAnalysis.transform","text":"","title":"transform()"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.QuadraticDiscriminantAnalysis","text":"[Source]","title":"QuadraticDiscriminantAnalysis"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.QuadraticDiscriminantAnalysis.decision_function","text":"","title":"decision_function()"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.QuadraticDiscriminantAnalysis.fit","text":"","title":"fit()"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.QuadraticDiscriminantAnalysis.predict","text":"","title":"predict()"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_log_proba","text":"","title":"predict_log_proba()"},{"location":"api/discriminant_analysis/#torchml.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_proba","text":"","title":"predict_proba()"},{"location":"api/kernel_approximations/","text":"torchml.kernel_approximations \u00b6 Classes \u00b6 torchml.kernel_approximations.RBFSampler \u00b6 [Source] Description \u00b6 RBFSampler constructs an approximate mapping for Random Kitchen Sinks. References \u00b6 Ali Rahimi and Benjamin Rechti's Weighted Sums of Random Kitchen Sinks paper The scikit-learn documentation page Arguments \u00b6 gamma (float, default=1.0) - Parameter of RBF kernel. n_components (int, default=100) - Dimensionality of the computed feature space. random_state (int, default=None) - Passed in seed that controls the generation of the random weights and random offset when fitting the training data. Example \u00b6 rbfsampler = RBFSampler () fit ( self , X : Tensor , y : Tensor = None ) \u00b6 Description \u00b6 Fit the model with training data X. Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates (None for unsupervised transformation). Example \u00b6 rbfsampler = RBFSampler () rbfsampler . fit ( X_train , y_train ) fit_transform ( self , X : Tensor , y : Tensor = None ) \u00b6 Description \u00b6 Fit and then transform X. Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates (None for unsupervised transformation). Example \u00b6 rbfsampler = RBFSampler () rbfsampler . fit_transform ( X_train , y_train ) transform ( self , X : Tensor ) \u00b6 Description \u00b6 Apply the approximate feature mapping to X. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 rbfsampler = RBFSampler () rbfsampler . fit ( X_train , y_train ) rbfsampler . transform ( X_train )","title":"torchml.kernel_approximations"},{"location":"api/kernel_approximations/#torchmlkernel_approximations","text":"","title":"torchml.kernel_approximations"},{"location":"api/kernel_approximations/#classes","text":"","title":"Classes"},{"location":"api/kernel_approximations/#torchml.kernel_approximations.RBFSampler","text":"[Source]","title":"RBFSampler"},{"location":"api/kernel_approximations/#torchml.kernel_approximations.RBFSampler.fit","text":"","title":"fit()"},{"location":"api/kernel_approximations/#torchml.kernel_approximations.RBFSampler.fit_transform","text":"","title":"fit_transform()"},{"location":"api/kernel_approximations/#torchml.kernel_approximations.RBFSampler.transform","text":"","title":"transform()"},{"location":"api/linear_model/","text":"torchml.linear_model \u00b6 Classes \u00b6 torchml.linear_model.LinearRegression \u00b6 [Source] Description \u00b6 Ordinary least-square model with bias term. Solves the following optimization problem in closed form: \\min_w \\vert \\vert Xw - y \\vert \\vert^2 References \u00b6 Wikipedia . Scikit-learn documentation. link . Arguments \u00b6 fit_intercept (bool) - Whether to fit a bias term. normalized (str) - Normalizes scheme to use. copy_X (bool) - Whether to copy the data X (else, it might be modified in-place). n_jobs (int) - Dummy to match the scikit-learn API. positive (bool) - Forces the coefficients to be positive when True (not implemented). Example \u00b6 linreg = LinearRegression ( fit_intercept = False ) fit ( self , X , y ) \u00b6 Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates. Example \u00b6 linreg = LinearRegression () linreg . fit ( X_train , y_train ) predict ( self , X ) \u00b6 Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 linreg = LinearRegression () linreg . fit ( X_train , y_train ) linreg . predict ( X_test ) torchml.linear_model.Ridge \u00b6 [Source] Description \u00b6 Linear regression with L2 penalty term. w = (X^TX + \\lambda I)^{-1}X^Ty w - weights of the linear regression with L2 penalty X - variates \u03bb - constant that multiplies the L2 term y - covariates The above equation is the closed-form solution for ridge's objective function \\min_w \\frac{1}{2} \\vert \\vert Xw - y \\vert \\vert^2 I + \\frac{1}{2} \\lambda \\vert \\vert w \\vert \\vert^2 References \u00b6 Arthur E. Hoerl and Robert W. Kennard's introduction to Ridge Regression paper Datacamp Lasso and Ridge Regression Tutorial tutorial The scikit-learn documentation page Arguments \u00b6 alpha (float, default=1.0) - Constant that multiplies the L2 term. alpha must be a non-negative float. fit_intercept (bool, default=False) - Whether or not to fit intercept in the model. normalize (bool, default=False) - If True, the regressors X will be normalized. normalize will be deprecated in the future. copy_X (bool, default=True) - If True, X will be copied. solver (string, default='auto') - Different solvers or algorithms to use. Example \u00b6 ridge = Ridge () fit ( self , X : Tensor , y : Tensor ) \u00b6 Description \u00b6 Compute the weights for the model given variates and covariates. Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates. Example \u00b6 ridge = Ridge () ridge . fit ( X_train , y_train ) predict ( self , X : Tensor ) \u00b6 Description \u00b6 Predict covariates by the trained model. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 ridge = Ridge () ridge . fit ( X_train , y_train ) ridge . predict ( X_test ) torchml.linear_model.Lasso \u00b6 [Source] Description \u00b6 Linear regression with L1 penalty term. \\min_w \\frac{1}{2m} \\vert \\vert Xw + b - y \\vert \\vert^2 I + \\frac{1}{2} \\lambda \\vert w \\vert m - number of input samples X - variates w - weights of the linear regression with L1 penalty b - intercept y - covariates \u03bb - constant that multiplies the L1 term Since lasso regression cannot derive into a closed-form equation, we used Cvxpylayers to construct a pytorch layer and directly compute the solution for the objective function above. References \u00b6 Robert Tibshirani's introduction to Lasso Regression paper Datacamp Lasso and Ridge Regression Tutorial tutorial The scikit-learn documentation page Arguments \u00b6 alpha (float, default=1.0) - Constant that multiplies the L1 term. alpha must be a non-negative float. fit_intercept (bool, default=False) - Whether or not to fit intercept in the model. positive (bool, default=False) - When set to True, forces the weights to be positive. require_grad (bool, default=False) - When set to True, tensor's require_grad will set to be true (useful if gradients need to be computed). Example \u00b6 lasso = Lasso () fit ( self , X : Tensor , y : Tensor ) \u00b6 Description \u00b6 Compute the weights for the model given variates and covariates. Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates. Example \u00b6 lasso = Lasso () lasso . fit ( X_train , y_train ) predict ( self , X : Tensor ) \u00b6 Description \u00b6 Predict covariates by the trained model. Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 lasso = Lasso () lasso . fit ( X_train , y_train ) lasso . predict ( X_test )","title":"torchml.linear_model"},{"location":"api/linear_model/#torchmllinear_model","text":"","title":"torchml.linear_model"},{"location":"api/linear_model/#classes","text":"","title":"Classes"},{"location":"api/linear_model/#torchml.linear_model.LinearRegression","text":"[Source]","title":"LinearRegression"},{"location":"api/linear_model/#torchml.linear_model.LinearRegression.fit","text":"","title":"fit()"},{"location":"api/linear_model/#torchml.linear_model.LinearRegression.predict","text":"","title":"predict()"},{"location":"api/linear_model/#torchml.linear_model.Ridge","text":"[Source]","title":"Ridge"},{"location":"api/linear_model/#torchml.linear_model.Ridge.fit","text":"","title":"fit()"},{"location":"api/linear_model/#torchml.linear_model.Ridge.predict","text":"","title":"predict()"},{"location":"api/linear_model/#torchml.linear_model.Lasso","text":"[Source]","title":"Lasso"},{"location":"api/linear_model/#torchml.linear_model.Lasso.fit","text":"","title":"fit()"},{"location":"api/linear_model/#torchml.linear_model.Lasso.predict","text":"","title":"predict()"},{"location":"api/naive_bayes/","text":"torchml.naive_bayes \u00b6 Classes \u00b6 torchml.naive_bayes.GaussianNB \u00b6 [Source] Description \u00b6 Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes' theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian. References \u00b6 H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS. Arguments \u00b6 priors : array-like of shape (n_classes,) Prior probabilities of the classes. If specified, the priors are not adjusted according to the data. var_smoothing : float, default=1e-9 Portion of the largest variance of all features that is added to variances for calculation stability. Example \u00b6 clf = GaussianNB () clf . fit ( X_train , y_train ) clf . predict ( X_test ) fit ( self , X : torch . Tensor , y : torch . Tensor ) \u00b6 Arguments \u00b6 X (Tensor) - Input variates. y (Tensor) - Target covariates. Example \u00b6 clf = GaussianNB () clf . fit ( X_train , y_train ) predict ( self , X : torch . Tensor ) \u00b6 Arguments \u00b6 X (Tensor) - Input variates. Example \u00b6 clf = GaussianNB () clf . fit ( X_train , y_train ) clf . predict ( X_test )","title":"torchml.naive_bayes"},{"location":"api/naive_bayes/#torchmlnaive_bayes","text":"","title":"torchml.naive_bayes"},{"location":"api/naive_bayes/#classes","text":"","title":"Classes"},{"location":"api/naive_bayes/#torchml.naive_bayes.GaussianNB","text":"[Source]","title":"GaussianNB"},{"location":"api/naive_bayes/#torchml.naive_bayes.GaussianNB.fit","text":"","title":"fit()"},{"location":"api/naive_bayes/#torchml.naive_bayes.GaussianNB.predict","text":"","title":"predict()"},{"location":"api/neighbors/","text":"torchml.neighbors \u00b6 Classes \u00b6 torchml.neighbors.NearestNeighbors \u00b6 Description \u00b6 Unsupervised learner for implementing neighbor searches. Implementation of scikit-learn's nearest neighbors APIs using PyTorch. References \u00b6 Fix, E. and Hodges, J.L. (1951) Discriminatory Analysis, Nonparametric Discrimination: Consistency Properties. Technical Report 4, USAF School of Aviation Medicine, Randolph Field. MIT 6.034 Artificial Intelligence, Fall 2010, 10. Introduction to Learning, Nearest Neighbors The scikit-learn documentation page for nearest neighbors. Referenced Implementation Arguments \u00b6 n_neighbors (int, default=5): Number of neighbors to use by default for kneighbors queries. radius (float, default=1.0): Range of parameter space to use by default for radius_neighbors queries. algorithm (string, default=\u2019auto\u2019): Dummy variable to mimic the sklearn API leaf_size (int, default=30): Dummy variable to mimic the sklearn API metric (str or callable, default=\u2019minkowski\u2019): No metric supprt right now, dummy variable and always minkowski p (int, default=2): No metric supprt right now, dummy variable and always 2 metric_paramsdict (default=None): Dummy variable to mimic the sklearn API n_jobs (int, default=None): Dummy variable to mimic the sklearn API Example \u00b6 import numpy as np import torchml as ml samples = np . array ([[ 1 , 0 , 0 ], [ 0 , 1 , 0 ], [ 0 , 5 , 0 ], [ 20 , 50 , 30 ]]) point = np . array ([[ 20 , 50 , 1 ]]) neigh = ml . neighbors . NearestNeighbors ( n_neighbors = 3 ) neigh . fit ( torch . from_numpy ( samples )) neigh . kneighbors ( torch . from_numpy ( point )) fit ( self , X : Tensor , y = None ) \u00b6 Description \u00b6 Initialize the class with training sets Arguments \u00b6 X (torch.Tensor): the training set y (torch.Tensor, default=None): dummy variable used to maintain the scikit-learn API consistency kneighbors ( self , X : Tensor , n_neighbors = None , return_distance = True ) -> < built - in function any > \u00b6 Description \u00b6 Computes the knearest neighbors and returns those k neighbors Arguments \u00b6 X (torch.Tensor): the target point n_neighbors (int, default=None): optional argument to respecify the parameter k in k nearest neighbors return_distance (bool, default=True): returns the distances to the neighbors if true torchml.neighbors.NearestCentroid \u00b6 Description \u00b6 Implementation of scikit-learn's Nearest centroid APIs using pytorch. Euclidean metric by default. References \u00b6 The scikit-learn [documentation page] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html) for nearest centroids. M. Thulasidas, \"Nearest Centroid: A Bridge between Statistics and Machine Learning,\" 2020 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE), 2020, pp. 9-16, doi: 10.1109/TALE48869.2020.9368396. Arguments \u00b6 metric (str or callable, default=\"euclidean\"): Metric to use for distance computation. Only Euclidiean metric is supported for now. shrink_threshold (float, default=None): Threshold for shrinking centroids to remove features. Not supported for now. Example \u00b6 import numpy as np import torchml as ml X = np . array ([[ 3 , 2 ],[ 1 , 8 ],[ 3 , 5 ],[ 9 , 7 ],[ 7 , 7 ]]) y = np . array ([ 9 , 1 , 9 , 8 , 3 ]) samp = np . array ([[ 6 , 1 ],[ 8 , 9 ],[ 5 , 3 ],[ 8 , 2 ],[ 9 , 8 ]]) torchX = torch . from_numpy ( X ) torchy = torch . from_numpy ( y ) centroid = ml . neighbors . NearestCentroid () centroid . fit ( torchX , torchy ) output = centroid . predict ( torch . from_numpy ( samp )) . numpy () fit ( self , X : Tensor , y : Tensor ) \u00b6 Description \u00b6 Fit the NearestCentroid model according to the given training data. Arguments \u00b6 X (torch.Tensor): array-like, sparse matrix of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features y (torch.Tensor): array-like of shape (n_samples,) Target values predict ( self , X : < built - in method tensor of type object at 0x10e8643b0 > ) -> < built - in method tensor of type object at 0x10e8643b0 > \u00b6 Description \u00b6 Computes the classes of the sample data Arguments \u00b6 X (torch.Tensor): the sample data, each with n-features Return \u00b6 (torch.Tensor): the predicted classes torchml.neighbors.KNeighborsClassifier \u00b6 Description \u00b6 Unsupervised learner for implementing KNN Classifier. References \u00b6 Fix, E. and Hodges, J.L. (1951) Discriminatory Analysis, Nonparametric Discrimination: Consistency Properties. Technical Report 4, USAF School of Aviation Medicine, Randolph Field. MIT 6.034 Artificial Intelligence, Fall 2010, 10. Introduction to Learning, Nearest Neighbors The scikit-learn documentation page for KNeighborsClassifier. Arguments \u00b6 n_neighbors (int, default=5): Number of neighbors to use by default for kneighbors queries. weights (str {'uniform', 'distance'} or callable, default='uniform'): 'uniform' : uniform weights. All points in each neighborhood are weighted equally. 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. [callable] : not implemented yet algorithm (str, default='auto'): Dummy variable to keep consistency with SKlearn's API, always 'brute' for now. leaf_size (int, default=30) Dummy variable to keep consistency with SKlearn's API. metric (str or callable, default=\u2019minkowski\u2019): No metric supprt right now, dummy variable and always minkowski p (int, default=2): No metric supprt right now, dummy variable and always 2 metric_paramsdict (default=None): Dummy variable to mimic the sklearn API n_jobs (int, default=None): Dummy variable to mimic the sklearn API Example \u00b6 import numpy as np import torchml as ml samples = np . array ([[ 0 ], [ 1 ], [ 2 ], [ 3 ]]) y = np . array ([ 0 , 0 , 1 , 1 ]) point = np . array ([ 1.1 ]) neigh = ml . neighbors . KNeighborsClassifier ( n_neighbors = 3 ) neigh . fit ( torch . from_numpy ( samples ), torch . from_numpy ( y )) neigh . predict ( torch . from_numpy ( point )) neigh . predict_proba ( torch . from_numpy ( point )) fit ( self , X : Tensor , y : Tensor ) \u00b6 Description \u00b6 Initialize the class with training sets Arguments \u00b6 X (torch.Tensor): the training set y (torch.Tensor, default=None): dummy variable used to maintain the scikit-learn API consistency kneighbors ( self , X : Tensor = None , n_neighbors : int = None , return_distance : bool = True ) -> Any \u00b6 Description \u00b6 Computes the knearest neighbors and returns those k neighbors Arguments \u00b6 X (torch.Tensor): the target point n_neighbors (int, default=None): optional argument to respecify the parameter k in k nearest neighbors return_distance (bool, default=True): returns the distances to the neighbors if true predict ( self , X : Tensor ) -> Tensor \u00b6 Description \u00b6 Predict the class labels for the provided data. Arguments \u00b6 X (torch.Tensor): the target point predict_proba ( self , X : Tensor ) -> Tensor \u00b6 Description \u00b6 Return probability estimates for the test data X. Arguments \u00b6 X (torch.Tensor): the target point","title":"torchml.neighbors"},{"location":"api/neighbors/#torchmlneighbors","text":"","title":"torchml.neighbors"},{"location":"api/neighbors/#classes","text":"","title":"Classes"},{"location":"api/neighbors/#torchml.neighbors.NearestNeighbors","text":"","title":"NearestNeighbors"},{"location":"api/neighbors/#torchml.neighbors.NearestNeighbors.fit","text":"","title":"fit()"},{"location":"api/neighbors/#torchml.neighbors.NearestNeighbors.kneighbors","text":"","title":"kneighbors()"},{"location":"api/neighbors/#torchml.neighbors.NearestCentroid","text":"","title":"NearestCentroid"},{"location":"api/neighbors/#torchml.neighbors.NearestCentroid.fit","text":"","title":"fit()"},{"location":"api/neighbors/#torchml.neighbors.NearestCentroid.predict","text":"","title":"predict()"},{"location":"api/neighbors/#torchml.neighbors.KNeighborsClassifier","text":"","title":"KNeighborsClassifier"},{"location":"api/neighbors/#torchml.neighbors.KNeighborsClassifier.fit","text":"","title":"fit()"},{"location":"api/neighbors/#torchml.neighbors.KNeighborsClassifier.kneighbors","text":"","title":"kneighbors()"},{"location":"api/neighbors/#torchml.neighbors.KNeighborsClassifier.predict","text":"","title":"predict()"},{"location":"api/neighbors/#torchml.neighbors.KNeighborsClassifier.predict_proba","text":"","title":"predict_proba()"},{"location":"tutorials/gaussian_nb/","text":"Gaussian Naive Bayes \u00b6 by Lucy X. Shi , on November 1, 2022. Introduction \u00b6 Gaussian Naive Bayes is a supervised learning algorithm based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. It is typically used for classification. Probabilistic Derivation \u00b6 Naive Bayes \u00b6 Bayes\u2019 theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$ : P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)} {P(x_1, \\dots, x_n)} Using the naive conditional independence assumption that P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y), for all $i$, this relationship is simplified to P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)} {P(x_1, \\dots, x_n)} Since $P(x_1, \\dots, x_n)$ is constant given the input, we can use the following classification rule: \\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align} and we can use the relative frequency of class $y$ in the training set to estimate $P(y)$. Gaussian Naive Bayes \u00b6 The likelihood of the features $P(x_i \\mid y)$ is assumed to be Gaussian: P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right) The parameters $\\sigma_y$ and $\\mu_y$ are estimated using maximum likelihood. In practice, instead of likelihood, we compute log likelihood. Implementation \u00b6 We first compute the mean and variance of each feature for all data points in the same class ( X[i] for the i th class): mu = torch.mean(X[i], dim=0) var = torch.var(X[i], dim=0, unbiased=False) Then we compute the joint log likelihood for the given data point to belong to each of the classes: n_ij = -0.5 * torch.sum(torch.log(2.0 * math.pi * self.var[i, :]), 0, True) n_ij -= 0.5 * torch.sum(((X - self.theta[i, :]) ** 2) / (self.var[i, :]), 1, True) Finally, we take an argmax for the joint log likelihood amongst all classes: return self.classes[joint_log_likelihood.argmax(1)] The torchml Interface \u00b6 First fit the classifier with training data, then make prediction using the classifier. clf = GaussianNB() clf.fit(X_train, y_train) pred = clf.predict(X_test) References \u00b6 scikit-learn H. Zhang (2004). The optimality of Naive Bayes . Proc. FLAIRS.","title":"Naive Bayes"},{"location":"tutorials/gaussian_nb/#gaussian-naive-bayes","text":"by Lucy X. Shi , on November 1, 2022.","title":"Gaussian Naive Bayes"},{"location":"tutorials/gaussian_nb/#introduction","text":"Gaussian Naive Bayes is a supervised learning algorithm based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. It is typically used for classification.","title":"Introduction"},{"location":"tutorials/gaussian_nb/#probabilistic-derivation","text":"","title":"Probabilistic Derivation"},{"location":"tutorials/gaussian_nb/#naive-bayes","text":"Bayes\u2019 theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$ : P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)} {P(x_1, \\dots, x_n)} Using the naive conditional independence assumption that P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y), for all $i$, this relationship is simplified to P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)} {P(x_1, \\dots, x_n)} Since $P(x_1, \\dots, x_n)$ is constant given the input, we can use the following classification rule: \\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align} and we can use the relative frequency of class $y$ in the training set to estimate $P(y)$.","title":"Naive Bayes"},{"location":"tutorials/gaussian_nb/#gaussian-naive-bayes_1","text":"The likelihood of the features $P(x_i \\mid y)$ is assumed to be Gaussian: P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right) The parameters $\\sigma_y$ and $\\mu_y$ are estimated using maximum likelihood. In practice, instead of likelihood, we compute log likelihood.","title":"Gaussian Naive Bayes"},{"location":"tutorials/gaussian_nb/#implementation","text":"We first compute the mean and variance of each feature for all data points in the same class ( X[i] for the i th class): mu = torch.mean(X[i], dim=0) var = torch.var(X[i], dim=0, unbiased=False) Then we compute the joint log likelihood for the given data point to belong to each of the classes: n_ij = -0.5 * torch.sum(torch.log(2.0 * math.pi * self.var[i, :]), 0, True) n_ij -= 0.5 * torch.sum(((X - self.theta[i, :]) ** 2) / (self.var[i, :]), 1, True) Finally, we take an argmax for the joint log likelihood amongst all classes: return self.classes[joint_log_likelihood.argmax(1)]","title":"Implementation"},{"location":"tutorials/gaussian_nb/#the-torchml-interface","text":"First fit the classifier with training data, then make prediction using the classifier. clf = GaussianNB() clf.fit(X_train, y_train) pred = clf.predict(X_test)","title":"The torchml Interface"},{"location":"tutorials/gaussian_nb/#references","text":"scikit-learn H. Zhang (2004). The optimality of Naive Bayes . Proc. FLAIRS.","title":"References"},{"location":"tutorials/linear_model/","text":"Ridge \u00b6 Introduction \u00b6 The \"Ridge\" is a supervised learning method on OLS adding a L2 penalty. This method of adding a small positive quantity (square of coefficients) to the OLS objective function is able to obtain an equation with smaller mean square error. Probablistic Derivation \u00b6 We first set up the maximum likelihood estimation equation: \\max_w p(w \\vert D) where D = \\{(x, y)\\}^N As we select our priors based on normal distribution (L2 penalty), the likelihood function could be reduced through the following equation: \\min_w \\frac{1}{2} \\vert \\vert Xw - y \\vert \\vert^2 I + \\frac{1}{2} \\lambda \\vert \\vert w \\vert \\vert^2 After simplification, the objective function becomes: \\min_w (X^TXw + w^TX^Ty)^T - X^Ty + \\lambda w While this equation is differentiable, we are able to deduct the closed form equation by taking the gradient of the objective function and set it to 0: w = (X^TX + \\lambda I)^{-1}X^Ty Implementation \u00b6 If user ask for the intercept term, we prepend the a column of 1 to input data X. if self.fit_intercept: X = torch.cat([torch.ones(X.shape[0], 1), X], dim=1) The weight of the model is calculated through the closed-form equation derived from the obejctive function. Similar to the implementation of Sklearn, intercept of the model is not penalized (not adding the ridge term). # L2 penalty term will not apply when alpha is 0 if self.alpha == 0: self.weight = torch.pinverse(X.T @ X) @ X.T @ y else: ridge = self.alpha * torch.eye(X.shape[1]) # intercept term is not penalized when fit_intercept is true if self.fit_intercept: ridge[0][0] = 0 self.weight = torch.pinverse((X.T @ X) + ridge) @ X.T @ y The torchml Interface \u00b6 First fit the model with training data, then make predictions. ridge = Ridge() ridge.fit(X_train, y_train) ridge.predict(X_test) References \u00b6 Arthur E. Hoerl and Robert W. Kennard's introduction to Ridge Regression paper The scikit-learn documentation page Lasso \u00b6 Introduction \u00b6 The \"Lasso\" is a supervised learning method used on OLS (ordinary least squares) that performs both variable selection and regularization. Mathematically, it consists of a linear model with added L1 penalty term. The objective function for Lasso is listed below: \\min_w \\frac{1}{2m} \\vert \\vert Xw + b - y \\vert \\vert^2 I + \\frac{1}{2} \\lambda \\vert w \\vert Implementation \u00b6 As Lasso objective function is not differentiable, it is hard to obtain a closed form equation that could be used directly to calculate the coefficients. In torchml, we utilizes Cvxpylayers to construct a differentiable convex optimization layer to compute the problem. One problem of using Cvxpylayers is that the coefficients are not able to reduce to 0 exactly, but Cvxpylayers is capable of minimizing the impact of certain features by assigning them extremely small coefficients). We first set up objective function of Lasso for further construction of cvxpylayer. if self.fit_intercept: loss = (1 / (2 * m)) * cp.sum(cp.square(X_param @ w + b - y_param)) else: loss = (1 / (2 * m)) * cp.sum(cp.square(X_param @ w - y_param)) penalty = alpha * cp.norm1(w) objective = loss + penalty Then we set up constraints, create the problem to minimize the objective function, and construct a pytorch layer. constraints = [] if self.positive: constraints = [w >= 0] prob = cp.Problem(cp.Minimize(objective), constraints) if self.fit_intercept: fit_lr = CvxpyLayer(prob, [X_param, y_param, alpha], [w, b]) else: fit_lr = CvxpyLayer(prob, [X_param, y_param, alpha], [w]) At last, we obtain weight and intercept (if asked by user) by fitting in the input training data into the constructed pytorch layer. if self.fit_intercept: self.weight, self.intercept = fit_lr(X, y, torch.tensor(self.alpha, dtype=torch.float64)) else: self.weight = fit_lr(X, y, torch.tensor(self.alpha, dtype=torch.float64)) self.weight = torch.stack(list(self.weight), dim=0) The torchml Interface \u00b6 First fit the model with training data, then make predictions. lasso = Lasso() lasso.fit(X_train, y_train) lasso.predict(X_test) References \u00b6 The scikit-learn tutorial page","title":"Linear Models"},{"location":"tutorials/linear_model/#ridge","text":"","title":"Ridge"},{"location":"tutorials/linear_model/#introduction","text":"The \"Ridge\" is a supervised learning method on OLS adding a L2 penalty. This method of adding a small positive quantity (square of coefficients) to the OLS objective function is able to obtain an equation with smaller mean square error.","title":"Introduction"},{"location":"tutorials/linear_model/#probablistic-derivation","text":"We first set up the maximum likelihood estimation equation: \\max_w p(w \\vert D) where D = \\{(x, y)\\}^N As we select our priors based on normal distribution (L2 penalty), the likelihood function could be reduced through the following equation: \\min_w \\frac{1}{2} \\vert \\vert Xw - y \\vert \\vert^2 I + \\frac{1}{2} \\lambda \\vert \\vert w \\vert \\vert^2 After simplification, the objective function becomes: \\min_w (X^TXw + w^TX^Ty)^T - X^Ty + \\lambda w While this equation is differentiable, we are able to deduct the closed form equation by taking the gradient of the objective function and set it to 0: w = (X^TX + \\lambda I)^{-1}X^Ty","title":"Probablistic Derivation"},{"location":"tutorials/linear_model/#implementation","text":"If user ask for the intercept term, we prepend the a column of 1 to input data X. if self.fit_intercept: X = torch.cat([torch.ones(X.shape[0], 1), X], dim=1) The weight of the model is calculated through the closed-form equation derived from the obejctive function. Similar to the implementation of Sklearn, intercept of the model is not penalized (not adding the ridge term). # L2 penalty term will not apply when alpha is 0 if self.alpha == 0: self.weight = torch.pinverse(X.T @ X) @ X.T @ y else: ridge = self.alpha * torch.eye(X.shape[1]) # intercept term is not penalized when fit_intercept is true if self.fit_intercept: ridge[0][0] = 0 self.weight = torch.pinverse((X.T @ X) + ridge) @ X.T @ y","title":"Implementation"},{"location":"tutorials/linear_model/#the-torchml-interface","text":"First fit the model with training data, then make predictions. ridge = Ridge() ridge.fit(X_train, y_train) ridge.predict(X_test)","title":"The torchml Interface"},{"location":"tutorials/linear_model/#references","text":"Arthur E. Hoerl and Robert W. Kennard's introduction to Ridge Regression paper The scikit-learn documentation page","title":"References"},{"location":"tutorials/linear_model/#lasso","text":"","title":"Lasso"},{"location":"tutorials/linear_model/#introduction_1","text":"The \"Lasso\" is a supervised learning method used on OLS (ordinary least squares) that performs both variable selection and regularization. Mathematically, it consists of a linear model with added L1 penalty term. The objective function for Lasso is listed below: \\min_w \\frac{1}{2m} \\vert \\vert Xw + b - y \\vert \\vert^2 I + \\frac{1}{2} \\lambda \\vert w \\vert","title":"Introduction"},{"location":"tutorials/linear_model/#implementation_1","text":"As Lasso objective function is not differentiable, it is hard to obtain a closed form equation that could be used directly to calculate the coefficients. In torchml, we utilizes Cvxpylayers to construct a differentiable convex optimization layer to compute the problem. One problem of using Cvxpylayers is that the coefficients are not able to reduce to 0 exactly, but Cvxpylayers is capable of minimizing the impact of certain features by assigning them extremely small coefficients). We first set up objective function of Lasso for further construction of cvxpylayer. if self.fit_intercept: loss = (1 / (2 * m)) * cp.sum(cp.square(X_param @ w + b - y_param)) else: loss = (1 / (2 * m)) * cp.sum(cp.square(X_param @ w - y_param)) penalty = alpha * cp.norm1(w) objective = loss + penalty Then we set up constraints, create the problem to minimize the objective function, and construct a pytorch layer. constraints = [] if self.positive: constraints = [w >= 0] prob = cp.Problem(cp.Minimize(objective), constraints) if self.fit_intercept: fit_lr = CvxpyLayer(prob, [X_param, y_param, alpha], [w, b]) else: fit_lr = CvxpyLayer(prob, [X_param, y_param, alpha], [w]) At last, we obtain weight and intercept (if asked by user) by fitting in the input training data into the constructed pytorch layer. if self.fit_intercept: self.weight, self.intercept = fit_lr(X, y, torch.tensor(self.alpha, dtype=torch.float64)) else: self.weight = fit_lr(X, y, torch.tensor(self.alpha, dtype=torch.float64)) self.weight = torch.stack(list(self.weight), dim=0)","title":"Implementation"},{"location":"tutorials/linear_model/#the-torchml-interface_1","text":"First fit the model with training data, then make predictions. lasso = Lasso() lasso.fit(X_train, y_train) lasso.predict(X_test)","title":"The torchml Interface"},{"location":"tutorials/linear_model/#references_1","text":"The scikit-learn tutorial page","title":"References"},{"location":"tutorials/nearest_centroid/","text":"Nearest Centroids \u00b6 Written by Bairen Chen on 10/18/2022. In this article, we will dive into a classification algorithm called Nearest Centroid and explain how to implement it with torchml. If you want to learn more about the NC Classifer's applications in research, ScienceDirect has a summary page. Note This tutorial is written for python users who are familiar with pytorch. Overview \u00b6 Introduction of NC Classifer and some intuition of how it works The implementation code step-be-step, making it easy for users to understand and use NC classifer in torchml Nearest Centroids Classifier & Intuition \u00b6 If you are familiar with the K-Nearest Neighbor Classifer, Nearest Centroids will seem quite familiar. In short, For each class in the data set that we train, we keep a representation of the class. Hence, if we have 'N' classes, we would have 'N' representations of the classes, and we call representations 'centroids'. \\\\ \\vec\\mu_\\ell = \\frac {1}{|C_\\ell|}\\underset{i\\in C_\\ell}{\\sum} \\vec {x}_i ' \\mu ' : per-class centroids ' \\vec{x} ' : training vector ' C_\\ell ' : the set of indices of samples belonging to class \\ell\\in\\mathbf{Y} Then when asked to predict with new data points, we compare each of the point with the representations of each class. We simiply find the 'Nearest' centroid and return the class that centroid represents as a the prediction. \\\\ \\hat{y} = \\argmin _{\\ell\\in \\mathbf{Y}}||\\vec{\\mu_\\ell} - \\vec{x}|| ' \\mu_\\ell ' : per-class centroids ' \\vec{x} ' : observation ' \\mathbf{Y} ' : all classes in traning set ' \\hat{y} ' : prediction classes Note: Often, we use mean/median of the training vectors to calculate centroids as they describe the class well, and Euclidean Distance to measure the distance between the centroids and the unseen data points for prediction. NC Classifer Implementation \u00b6 This section breaks down step-by-step the NC Classifier implementation with example code. Import torchml import torch import torchml as ml Note: importing numpy for numpy arrays demonstration. Creating dataset X = torch . tensor ([[ - 1 , - 1 ], [ - 2 , - 1 ], [ - 3 , - 2 ], [ 1 , 1 ], [ 2 , 1 ], [ 3 , 2 ]]) y = torch . tensor ([ 1 , 1 , 1 , 2 , 2 , 2 ]) Here we have: 'X' is a 6x2 training vector: we have 6 samples in our data set and each has 2 features. 'y' is a 1x6 vector of target values: we have 6 target values for the 6 samples in our training vector X. Creating model & Fit centroid = ml . neighbors . NearestCentroid () centroid . fit ( X , y ) Next, we instantiate the Nearest Centroid Classifer instance in torchml, call it 'centroid'. Then, we call the fit function in the Nearest Centroid class to fit the Nearest Centroid model according to the given data 'X' and 'y'. Predict test = torch . tensor ([[ - 0.8 , - 1 ]]) print ( centroid . predict ( test )) Output [ 1 ] Finally, we want to use our NC Classifer model to predict the class of a test vector called 'test'. Calling the predict function in the Nearest Centroid class will perform the classification and output the predicted class. Conclusion \u00b6 Having explained the background of Nearest Centroid and its code implementation with torchml, I hope this tutorial will be helpful to those who are interested in using Nearest Centroid for their classification tasks. References \u00b6 scikit-learn Nearest Centroid Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html Wikipedia Page on Nearest Centroid. https://en.wikipedia.org/wiki/Nearest_centroid_classifier","title":"Nearest Centroid"},{"location":"tutorials/nearest_centroid/#nearest-centroids","text":"Written by Bairen Chen on 10/18/2022. In this article, we will dive into a classification algorithm called Nearest Centroid and explain how to implement it with torchml. If you want to learn more about the NC Classifer's applications in research, ScienceDirect has a summary page. Note This tutorial is written for python users who are familiar with pytorch.","title":"Nearest Centroids"},{"location":"tutorials/nearest_centroid/#overview","text":"Introduction of NC Classifer and some intuition of how it works The implementation code step-be-step, making it easy for users to understand and use NC classifer in torchml","title":"Overview"},{"location":"tutorials/nearest_centroid/#nearest-centroids-classifier-intuition","text":"If you are familiar with the K-Nearest Neighbor Classifer, Nearest Centroids will seem quite familiar. In short, For each class in the data set that we train, we keep a representation of the class. Hence, if we have 'N' classes, we would have 'N' representations of the classes, and we call representations 'centroids'. \\\\ \\vec\\mu_\\ell = \\frac {1}{|C_\\ell|}\\underset{i\\in C_\\ell}{\\sum} \\vec {x}_i ' \\mu ' : per-class centroids ' \\vec{x} ' : training vector ' C_\\ell ' : the set of indices of samples belonging to class \\ell\\in\\mathbf{Y} Then when asked to predict with new data points, we compare each of the point with the representations of each class. We simiply find the 'Nearest' centroid and return the class that centroid represents as a the prediction. \\\\ \\hat{y} = \\argmin _{\\ell\\in \\mathbf{Y}}||\\vec{\\mu_\\ell} - \\vec{x}|| ' \\mu_\\ell ' : per-class centroids ' \\vec{x} ' : observation ' \\mathbf{Y} ' : all classes in traning set ' \\hat{y} ' : prediction classes Note: Often, we use mean/median of the training vectors to calculate centroids as they describe the class well, and Euclidean Distance to measure the distance between the centroids and the unseen data points for prediction.","title":"Nearest Centroids Classifier &amp; Intuition"},{"location":"tutorials/nearest_centroid/#nc-classifer-implementation","text":"This section breaks down step-by-step the NC Classifier implementation with example code. Import torchml import torch import torchml as ml Note: importing numpy for numpy arrays demonstration. Creating dataset X = torch . tensor ([[ - 1 , - 1 ], [ - 2 , - 1 ], [ - 3 , - 2 ], [ 1 , 1 ], [ 2 , 1 ], [ 3 , 2 ]]) y = torch . tensor ([ 1 , 1 , 1 , 2 , 2 , 2 ]) Here we have: 'X' is a 6x2 training vector: we have 6 samples in our data set and each has 2 features. 'y' is a 1x6 vector of target values: we have 6 target values for the 6 samples in our training vector X. Creating model & Fit centroid = ml . neighbors . NearestCentroid () centroid . fit ( X , y ) Next, we instantiate the Nearest Centroid Classifer instance in torchml, call it 'centroid'. Then, we call the fit function in the Nearest Centroid class to fit the Nearest Centroid model according to the given data 'X' and 'y'. Predict test = torch . tensor ([[ - 0.8 , - 1 ]]) print ( centroid . predict ( test )) Output [ 1 ] Finally, we want to use our NC Classifer model to predict the class of a test vector called 'test'. Calling the predict function in the Nearest Centroid class will perform the classification and output the predicted class.","title":"NC Classifer Implementation"},{"location":"tutorials/nearest_centroid/#conclusion","text":"Having explained the background of Nearest Centroid and its code implementation with torchml, I hope this tutorial will be helpful to those who are interested in using Nearest Centroid for their classification tasks.","title":"Conclusion"},{"location":"tutorials/nearest_centroid/#references","text":"scikit-learn Nearest Centroid Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html Wikipedia Page on Nearest Centroid. https://en.wikipedia.org/wiki/Nearest_centroid_classifier","title":"References"},{"location":"tutorials/neighbors/","text":"Nearest Neighbors \u00b6 Written by Zhiheng Zhang on 11/10/2022. Introduction \u00b6 torchml.neighbors currently supports unsupervised learnings on classification problem. It currently supports K Nearest Neighbors classification with torchml.neighbors.NearestNeighbors that implement sklearn.neighbors.NearestNeighbors 's brute force solution with torchml. Probabilistic Derivation \u00b6 K Nearest Neighbors classification \u00b6 The principle behind Nearest Neighbors algorithms is, given a distance function and a new test point x , the algorithm find k closest samples in the known sample set, and use them to estimate the x . The number k can be user-defined and tuned according to the particular problem. The distance function can be any arbitrary metric function, and standard Euclidean distance is the most common choice. One important thing about this algorithm is that its not based on any probabilistic framework, but the algorithm is able to estimate probability for each class given a test point x and its k neighbors. Given a dataset with n samples and b distinct classes, and a new point x we wish to classify: \\{x_i, y_i\\}, i=1,2....n, y_i \\in \\{c_1, c_2, c_3... c_b\\} We calculate the number of samples that fall into a class for all classes: \\{n_a, a=1,2,3...b\\}, \\Sigma_{a=1}^{b}n_a = n We first find the k nearest neighbors of x : \\{x_j, y_j\\}, i=1,2....k, y_j \\in \\{c_1, c_2, c_3... c_c\\} We then count the number of points in the k neighbors that are in the class (c$: \\{nk_a, a=1,2,3...b\\}, \\Sigma_{a=1}^{b}nk_a = k The probability that x is of class c_c is simply: P(c_c | x)= {nk_c\\over k} This estimation is often accurate in practice, even though the algorithm is not built with probability in mind. KNN from a bayesian stand point \u00b6 Even though the KNN algorithm is not built on top of probabilistic framework, we can gain intuition behind its shockingly good estimation by framing it in the bayesian framework. What we want is: P(c_c | x), c=1,2,3...b and in bayesian terms, what we need is: P(c_c | x) = {{P(x | c_c)*P(c_c)} \\over {P(x)}} Given nothing but our samples, P(c_c) , or the prior, is simply n_c \\over n P(x) is the probabilistic density of random variable x , and we need to borrow some knowledge from density estimation for this analysis: Since we don't know P(x) , we need to conduct discrete trials on P(x) . Suppose that the density P(x) lies in a D-Dimensional space, and we assume it to be Euclidean. We conduct trials in this space by drawing n points on it according to P(x) (these n points are our samples). By principle of locality, for a given point x_t we've drawn on the space, we can assume that the density have some correlations with points in the small space surrounding it. Let's draw a small sphere around the point, and name the space in the sphere R . The total probability that a test point can end up inside R is the sum of probability that a point can be in a point in R over all the small points in R , or the probability mass of P(x) in R : P_{in R} = {\\int_{R} P(x)dx} For the n samples we gathered, each sample has a probability P_{in R} of being inside R , then the total number of k points that successfully end up in R can be modeled using binomial distribution: Bin(k|n,P_{in R}) = {n! \\over {k!(n-k)!}}{P_{in R}^k}{(1-P_{in R})}^{n-k} We also have: E(k) = n*P_{in R} P_{in R} = {{E(k)} \\over n} For our algorithm we supply the parameter k , so we can just sub in our well-chosen k instead of the expectation, which gives us: k \\approx n*P_{in R} P_{in R} \\approx {k \\over n} We further assume that R is quite small, thus P(x) changes very little inside R , and we assume P(x) to follow a uniform distribution, then we can derive that: P_{in R} \\approx P(x)V where V is the volume of R . Then our final estimation of P(x) will be: P(x) = {{k}\\over{nV}} We repeat the process for a specific class c_c , and we will get: P(x|c_c) = {{nk_c}\\over{n_c V}} substitute both P(x|c_c) = {{nk_c}\\over{n_c V}} and P(x) = {{k}\\over{nV}} into our bayesian, we will get: P(c_c | x)= {nk_c\\over k}. Algorithmic Implementation \u00b6 Given a new sample, the brute-force algorithm is to: 1. Calculate all pairwise distances between the sample point and the labeled examples 2. Find the k neighboring samples with the least k smallest distances 3. For each class, obtain the ratio of number of points in that class in the k neighbors and the number k , and that ratio will be the probability that the new sample belongs to this class. The torchml Interface \u00b6 import numpy as np import torchml as ml samples = np . array ([[ 0 ], [ 1 ], [ 2 ], [ 3 ]]) y = np . array ([ 0 , 0 , 1 , 1 ]) point = np . array ([ 1.1 ]) neigh = ml . neighbors . KNeighborsClassifier ( n_neighbors = 3 ) neigh . fit ( torch . from_numpy ( samples ), torch . from_numpy ( y )) neigh . predict ( torch . from_numpy ( point )) # returns the most likely class label neigh . predict_proba ( torch . from_numpy ( point )) # returns all the class probabilities References \u00b6 Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg. MIT Lecture on KNN","title":"Nearest Neighbors"},{"location":"tutorials/neighbors/#nearest-neighbors","text":"Written by Zhiheng Zhang on 11/10/2022.","title":"Nearest Neighbors"},{"location":"tutorials/neighbors/#introduction","text":"torchml.neighbors currently supports unsupervised learnings on classification problem. It currently supports K Nearest Neighbors classification with torchml.neighbors.NearestNeighbors that implement sklearn.neighbors.NearestNeighbors 's brute force solution with torchml.","title":"Introduction"},{"location":"tutorials/neighbors/#probabilistic-derivation","text":"","title":"Probabilistic Derivation"},{"location":"tutorials/neighbors/#k-nearest-neighbors-classification","text":"The principle behind Nearest Neighbors algorithms is, given a distance function and a new test point x , the algorithm find k closest samples in the known sample set, and use them to estimate the x . The number k can be user-defined and tuned according to the particular problem. The distance function can be any arbitrary metric function, and standard Euclidean distance is the most common choice. One important thing about this algorithm is that its not based on any probabilistic framework, but the algorithm is able to estimate probability for each class given a test point x and its k neighbors. Given a dataset with n samples and b distinct classes, and a new point x we wish to classify: \\{x_i, y_i\\}, i=1,2....n, y_i \\in \\{c_1, c_2, c_3... c_b\\} We calculate the number of samples that fall into a class for all classes: \\{n_a, a=1,2,3...b\\}, \\Sigma_{a=1}^{b}n_a = n We first find the k nearest neighbors of x : \\{x_j, y_j\\}, i=1,2....k, y_j \\in \\{c_1, c_2, c_3... c_c\\} We then count the number of points in the k neighbors that are in the class (c$: \\{nk_a, a=1,2,3...b\\}, \\Sigma_{a=1}^{b}nk_a = k The probability that x is of class c_c is simply: P(c_c | x)= {nk_c\\over k} This estimation is often accurate in practice, even though the algorithm is not built with probability in mind.","title":"K Nearest Neighbors classification"},{"location":"tutorials/neighbors/#knn-from-a-bayesian-stand-point","text":"Even though the KNN algorithm is not built on top of probabilistic framework, we can gain intuition behind its shockingly good estimation by framing it in the bayesian framework. What we want is: P(c_c | x), c=1,2,3...b and in bayesian terms, what we need is: P(c_c | x) = {{P(x | c_c)*P(c_c)} \\over {P(x)}} Given nothing but our samples, P(c_c) , or the prior, is simply n_c \\over n P(x) is the probabilistic density of random variable x , and we need to borrow some knowledge from density estimation for this analysis: Since we don't know P(x) , we need to conduct discrete trials on P(x) . Suppose that the density P(x) lies in a D-Dimensional space, and we assume it to be Euclidean. We conduct trials in this space by drawing n points on it according to P(x) (these n points are our samples). By principle of locality, for a given point x_t we've drawn on the space, we can assume that the density have some correlations with points in the small space surrounding it. Let's draw a small sphere around the point, and name the space in the sphere R . The total probability that a test point can end up inside R is the sum of probability that a point can be in a point in R over all the small points in R , or the probability mass of P(x) in R : P_{in R} = {\\int_{R} P(x)dx} For the n samples we gathered, each sample has a probability P_{in R} of being inside R , then the total number of k points that successfully end up in R can be modeled using binomial distribution: Bin(k|n,P_{in R}) = {n! \\over {k!(n-k)!}}{P_{in R}^k}{(1-P_{in R})}^{n-k} We also have: E(k) = n*P_{in R} P_{in R} = {{E(k)} \\over n} For our algorithm we supply the parameter k , so we can just sub in our well-chosen k instead of the expectation, which gives us: k \\approx n*P_{in R} P_{in R} \\approx {k \\over n} We further assume that R is quite small, thus P(x) changes very little inside R , and we assume P(x) to follow a uniform distribution, then we can derive that: P_{in R} \\approx P(x)V where V is the volume of R . Then our final estimation of P(x) will be: P(x) = {{k}\\over{nV}} We repeat the process for a specific class c_c , and we will get: P(x|c_c) = {{nk_c}\\over{n_c V}} substitute both P(x|c_c) = {{nk_c}\\over{n_c V}} and P(x) = {{k}\\over{nV}} into our bayesian, we will get: P(c_c | x)= {nk_c\\over k}.","title":"KNN from a bayesian stand point"},{"location":"tutorials/neighbors/#algorithmic-implementation","text":"Given a new sample, the brute-force algorithm is to: 1. Calculate all pairwise distances between the sample point and the labeled examples 2. Find the k neighboring samples with the least k smallest distances 3. For each class, obtain the ratio of number of points in that class in the k neighbors and the number k , and that ratio will be the probability that the new sample belongs to this class.","title":"Algorithmic Implementation"},{"location":"tutorials/neighbors/#the-torchml-interface","text":"import numpy as np import torchml as ml samples = np . array ([[ 0 ], [ 1 ], [ 2 ], [ 3 ]]) y = np . array ([ 0 , 0 , 1 , 1 ]) point = np . array ([ 1.1 ]) neigh = ml . neighbors . KNeighborsClassifier ( n_neighbors = 3 ) neigh . fit ( torch . from_numpy ( samples ), torch . from_numpy ( y )) neigh . predict ( torch . from_numpy ( point )) # returns the most likely class label neigh . predict_proba ( torch . from_numpy ( point )) # returns all the class probabilities","title":"The torchml Interface"},{"location":"tutorials/neighbors/#references","text":"Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg. MIT Lecture on KNN","title":"References"}]}